{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "998b3d36-bb04-4f42-8e86-3e952efd85f7",
   "metadata": {},
   "source": [
    "# Introduction to Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0103e8b9-6bee-4342-8efb-43bd04d81f78",
   "metadata": {},
   "source": [
    "## What is Pandas?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d773a5-b4bb-4875-9352-3c09753ed9fd",
   "metadata": {},
   "source": [
    "Pandas is an open-source library providing high-performance, easy-to-use data structures, and data analysis tools for Python. Its primary data structures, the Series and DataFrame, allow you to handle and analyze data in a way tailored to data science needs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552635a7-0b44-4a93-985d-945a1fa85b66",
   "metadata": {},
   "source": [
    "# Benefits of Using Pandas\n",
    "\n",
    "Pandas is a powerful data manipulation library in Python that offers several key benefits:\n",
    "\n",
    "1. **Structured Data Handling:** Pandas provides data structures like DataFrames and Series, which allow you to work with structured data in a tabular form. This makes it easy to read, manipulate, and analyze data, similar to working with a spreadsheet.\n",
    "\n",
    "2. **Data Cleaning and Transformation:** Pandas offers a wide range of functions for data cleaning, handling missing values, and transforming data. It simplifies tasks such as data imputation, filtering, and merging.\n",
    "\n",
    "![Illustration of Data Cleaning](images/data_cleaning_cycle.png)\n",
    "\n",
    "3. **Efficient Data Loading and Storage:** Pandas supports reading data from various file formats (CSV, Excel, SQL databases) and can export data to different formats. It efficiently handles large datasets and optimizes memory usage.\n",
    "\n",
    "4. **Data Analysis and Exploration:** With Pandas, you can perform data analysis tasks like aggregation, grouping, and statistical operations. It provides tools for summarizing and visualizing data, making it easier to gain insights.\n",
    "\n",
    "5. **Integration with Other Libraries:** Pandas seamlessly integrates with other data science libraries like NumPy, Matplotlib, and Scikit-Learn. This allows you to combine data manipulation, analysis, and visualization in a cohesive workflow.\n",
    "\n",
    "6. **Time Series Data Handling:** Pandas has excellent support for time series data, making it suitable for financial, scientific, and IoT applications.\n",
    "\n",
    "7. **Community Support:** Pandas has a large and active community, which means you can find plenty of resources, tutorials, and solutions to common data manipulation challenges.\n",
    "\n",
    "**Image Context**: The provided image illustrates a common use case of Pandas, showing how it simplifies data cleaning and transformation tasks. It can visually represent actions like removing missing values, filtering data, and performing transformations, helping users understand the power of Pandas in data preparation.\n",
    "\n",
    "By leveraging Pandas, data scientists and analysts can streamline their data processing workflows and focus on deriving meaningful insights from their datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a91883-d9c0-4a3a-9278-8579059688b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f451721e-260f-42b8-b8ed-59b3244efa6e",
   "metadata": {},
   "source": [
    "# Loading Data into Pandas DataFrames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ac3715-1a87-4b35-a0ed-d2e3827b4266",
   "metadata": {},
   "source": [
    "## Creating a DataFrame from Lists and Dictionaries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d700df0c-d087-475d-845b-decff41542a4",
   "metadata": {},
   "source": [
    "Pandas provides various functions to read data, one of the most common formats being the CSV (Comma-Separated Values) file. CSV files are plain text files that contain data separated by commas (or sometimes other delimiters)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d88a24b-0ff9-4e11-9a8b-72bbb559d83e",
   "metadata": {},
   "source": [
    "> **Overview**\n",
    ">\n",
    "> DataFrames are a fundamental data structure in pandas, a popular Python library for data manipulation and analysis. They are used to represent and work with tabular data, much like a spreadsheet or database table. This markdown block provides an explanation of how to create DataFrames in pandas using two different methods: from a list and from a dictionary.\n",
    "\n",
    "> **Creating a DataFrame from a List**\n",
    ">\n",
    "> To create a DataFrame from a list, you can use the `pd.DataFrame()` constructor provided by the pandas library. In the example, we have a list called `data_list` containing sublists, where each sublist represents a row of data. We also specify column names as 'Name' and 'Age' using the `columns` parameter.\n",
    ">\n",
    "> ```python\n",
    "> data_list = [['Alex', 10], ['Ron', 15], ['Jane', 13]]\n",
    "> df_from_list = pd.DataFrame(data_list, columns=['Name', 'Age'])\n",
    "> ```\n",
    ">\n",
    "> The `pd.DataFrame()` constructor takes the list `data_list` and converts it into a DataFrame object with labeled columns. This allows us to work with the data in a structured tabular format. The resulting DataFrame, `df_from_list`, looks like this:\n",
    ">\n",
    "> |   | Name | Age |\n",
    "> |---|------|-----|\n",
    "> | 0 | Alex | 10  |\n",
    "> | 1 | Ron  | 15  |\n",
    "> | 2 | Jane | 13  |\n",
    ">\n",
    "> Each sublist in `data_list` corresponds to a row in the DataFrame, and the column names 'Name' and 'Age' are assigned to the respective columns.\n",
    "\n",
    "> **Creating a DataFrame from a Dictionary**\n",
    ">\n",
    "> Creating a DataFrame from a dictionary is another common approach. In this case, the keys of the dictionary become the column names, and the values associated with each key become the data for that column. In the example, we have a dictionary called `data_dict` with 'Name' and 'Age' as keys.\n",
    ">\n",
    "> ```python\n",
    "> data_dict = {'Name': ['Alex', 'Ron', 'Jane'], 'Age': [10, 15, 13]}\n",
    "> df_from_dict = pd.DataFrame(data_dict)\n",
    "> ```\n",
    ">\n",
    "> Using the `pd.DataFrame()` constructor with `data_dict`, we create the DataFrame `df_from_dict`. The resulting DataFrame looks like this:\n",
    ">\n",
    "> |   | Name | Age |\n",
    "> |---|------|-----|\n",
    "> | 0 | Alex | 10  |\n",
    "> | 1 | Ron  | 15  |\n",
    "> | 2 | Jane | 13  |\n",
    ">\n",
    "> The keys of the dictionary ('Name' and 'Age') become the column names, and the corresponding values form the data for each column. This method is convenient when you have data organized in a dictionary and want to convert it into a DataFrame for analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5853b798-b454-40af-8ffd-7f2142b6ea47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating a DataFrame from a list\n",
    "data_list = [['Alex', 10], ['Ron', 15], ['Jane', 13]]\n",
    "df_from_list = pd.DataFrame(data_list, columns=['Name', 'Age'])\n",
    "print(df_from_list)\n",
    "\n",
    "# Creating a DataFrame from a dictionary\n",
    "data_dict = {'Name': ['Alex', 'Ron', 'Jane'], 'Age': [10, 15, 13]}\n",
    "df_from_dict = pd.DataFrame(data_dict)\n",
    "df_from_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca1ebc6-21ef-46f4-a86d-23a6654de239",
   "metadata": {},
   "source": [
    "## Creating DataFrames from a CSV File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89453a3-03e6-4e3e-a981-8c8173300441",
   "metadata": {},
   "source": [
    "> **Overview**\n",
    ">\n",
    "> In this markdown block, we explore how to load CSV data into a pandas DataFrame and perform basic viewing operations on the loaded data.\n",
    "\n",
    "> **Loading CSV Data into a DataFrame**\n",
    ">\n",
    "> To load CSV data into a pandas DataFrame, you can use the `pd.read_csv()` function provided by the pandas library. In the example, we load data from a CSV file named 'sample_data.csv' into a DataFrame called `df_csv`.\n",
    ">\n",
    "> ```python\n",
    "> df_csv = pd.read_csv('sample_data.csv')\n",
    "> ```\n",
    ">\n",
    "> The `pd.read_csv()` function reads the contents of the CSV file and converts it into a DataFrame, allowing you to work with the tabular data.\n",
    "\n",
    "> **Displaying the Top 5 Lines of Loaded Data**\n",
    ">\n",
    "> After loading data into a DataFrame, you can use the `.head()` method to display the top few rows of the DataFrame. By default, it shows the first 5 rows, but you can specify a different number of rows by passing an integer as an argument.\n",
    ">\n",
    "> ```python\n",
    "> df_csv.head()\n",
    "> ```\n",
    ">\n",
    "> The `head()` method is useful for quickly inspecting the structure and content of your DataFrame. It displays the first few rows, along with the column names and data, providing an overview of the loaded data.\n",
    "\n",
    "> **Additional Basic Viewing Functionality**\n",
    ">\n",
    "> In addition to `.head()`, pandas offers several other basic viewing and exploration methods to better understand your data. Some of these include:\n",
    ">\n",
    "> - `.tail()`: Similar to `.head()`, but displays the last few rows of the DataFrame.\n",
    "> - `.info()`: Provides a summary of the DataFrame's structure, including data types, non-null counts, and memory usage.\n",
    "> - `.describe()`: Generates descriptive statistics for numerical columns, such as mean, standard deviation, and quartiles.\n",
    "> - `.shape`: Returns a tuple representing the dimensions of the DataFrame (number of rows, number of columns).\n",
    "> - `.columns`: Returns a list of column names in the DataFrame.\n",
    ">\n",
    "> These functions are essential for data exploration and initial analysis, helping you gain insights into your data's characteristics and quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fd958f-ff07-46a9-9d4c-acfcbf053e78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load CSV data into a DataFrame\n",
    "df_csv = pd.read_csv('sample_data.csv')\n",
    "\n",
    "# Display the top 5 lines of the loaded data\n",
    "print(\"Top 5 lines of the loaded data:\")\n",
    "print(df_csv.head())\n",
    "\n",
    "# Display information about the DataFrame including data types and non-null counts\n",
    "print(\"\\nInformation about the DataFrame:\")\n",
    "print(df_csv.info())\n",
    "\n",
    "# Generate descriptive statistics for numerical columns\n",
    "print(\"\\nDescriptive statistics for numerical columns:\")\n",
    "print(df_csv.describe())\n",
    "\n",
    "# Get the list of column names in the DataFrame\n",
    "print(\"\\nColumn names in the DataFrame:\")\n",
    "print(df_csv.columns)\n",
    "\n",
    "# Get the dimensions of the DataFrame (number of rows and columns)\n",
    "print(\"\\nDimensions of the DataFrame:\")\n",
    "print(df_csv.shape)\n",
    "\n",
    "# Display the last 5 lines of the loaded data\n",
    "print(\"\\nLast 5 lines of the loaded data:\")\n",
    "print(df_csv.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183f3936-9135-4844-ae2e-506efd276a4c",
   "metadata": {},
   "source": [
    "# Data Cleaning and Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e786be-5288-41f0-beec-c99714dd3a60",
   "metadata": {},
   "source": [
    "## Handling Missing Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd3d404-da62-453d-8535-3ed7d7d4187d",
   "metadata": {},
   "source": [
    "> **Overview**\n",
    ">\n",
    "\n",
    "> **Creating a DataFrame with Missing Values**\n",
    ">\n",
    "> We begin by creating a DataFrame called `df_na` that contains missing values (represented as NaN) using the NumPy library. The DataFrame `df_na` has columns 'A', 'B', and 'C', with missing values in specific cells.\n",
    "\n",
    "> **Displaying Missing Values**\n",
    ">\n",
    "> We print the original DataFrame `df_na` to display its content, including the missing values.\n",
    "\n",
    "> **Dropping Rows with Missing Values**\n",
    ">\n",
    "> Next, we use the `.dropna()` method to remove rows containing missing values from the DataFrame. The resulting DataFrame, `df_no_na`, contains only rows with complete data.\n",
    "\n",
    "> **Filling Missing Values with a Placeholder**\n",
    ">\n",
    "> We use the `.fillna()` method to replace missing values with a placeholder value, in this case, 0. The resulting DataFrame, `df_filled`, has missing values filled with zeros.\n",
    "\n",
    "> **Checking for Missing and Non-Missing Values**\n",
    ">\n",
    "> We perform checks to determine the presence of missing values in the original DataFrame `df_na`. We calculate the number of missing values for each column using `.isna().sum()` and the number of non-missing values using `.count()`.\n",
    "\n",
    "> **Checking for Any Missing Values**\n",
    ">\n",
    "> Lastly, we use `.isna().any().any()` to check if any missing values exist in the DataFrame. This provides a binary answer to whether the DataFrame contains any missing data.\n",
    "\n",
    "These operations demonstrate common techniques for handling missing values in a pandas DataFrame and assessing the data's completeness and quality.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4c1d0c-6c83-4376-ab97-52daf688353e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating a DataFrame with missing values\n",
    "import numpy as np\n",
    "data_with_na = {'A': [1, 2, np.nan], 'B': [4, np.nan, 6], 'C': [7, 8, 9]}\n",
    "df_na = pd.DataFrame(data_with_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e343ab6-fd86-4267-a610-f9ec5f2f60e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Displaying missing values\n",
    "print(\"DataFrame with Missing Values:\\n\", df_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0ba9f3-6b6d-4a7b-b599-56556f03f46e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dropping rows containing missing values\n",
    "df_no_na = df_na.dropna()\n",
    "print(\"\\nAfter dropping rows with missing values:\\n\", df_no_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633b6e75-7323-4380-bf30-d4a1e5b884a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filling missing values with a placeholder (e.g., 0)\n",
    "df_filled = df_na.fillna(0)\n",
    "print(\"\\nAfter filling missing values with 0:\\n\", df_filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330ba3ff-a189-4bc4-8971-f00d86420843",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = df_na.isna().sum()\n",
    "print(\"\\nMissing values in the original DataFrame:\\n\", missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a30471-a5ac-4e25-9985-7c247260431d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check for non-missing values\n",
    "non_missing_values = df_na.count()\n",
    "print(\"\\nNon-missing values in the original DataFrame:\\n\", non_missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec613ee1-b569-4728-ab3e-b1d8c6240f08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check if any missing values exist in the DataFrame\n",
    "has_missing_values = df_na.isna().any().any()\n",
    "print(\"\\nDoes the DataFrame have any missing values?\\n\", has_missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee590484-16b3-4b42-aaab-c73426c13c9a",
   "metadata": {},
   "source": [
    "## Transforming Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7110a705-c0d6-4699-9a50-f1eaca329edb",
   "metadata": {},
   "source": [
    "Data often requires transformation, whether to normalize it, categorize it, or simply derive new insights. Here's how you can transform data using Pandas:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2945b766-c64c-44b7-b81d-18107118b95d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Applying a function to a column to double the age\n",
    "df_from_list['Doubled Age'] = df_from_list['Age'].apply(lambda age: age * 2)\n",
    "print(df_from_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5784be0d-df26-418c-ac04-9fdaa0b3290f",
   "metadata": {},
   "source": [
    "# Data Selection and Filtering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ea2916-6b7f-4f90-acf3-0692e67369e6",
   "metadata": {},
   "source": [
    "## Selecting Columns and Rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed53206-dd79-4f44-8f17-fce51d4b3a74",
   "metadata": {
    "tags": []
   },
   "source": [
    "> **Overview**\n",
    ">\n",
    "> In this markdown block, we explore various data selection and slicing techniques in pandas DataFrame. The code block demonstrates how to select single columns as Series, specific columns as DataFrames, filter rows based on conditions, and use the `.loc` and `.iloc` methods for precise data selection.\n",
    "\n",
    "> **Selecting a Single Column**\n",
    ">\n",
    "> We start by selecting a single column, 'Name', from the DataFrame `df_from_list`. This operation creates a pandas Series containing only the 'Name' column.\n",
    "\n",
    "> **Selecting Specific Columns**\n",
    ">\n",
    "> Next, we demonstrate how to select specific columns, 'Name' and 'Age', from the DataFrame. This operation creates a new DataFrame called `subset` containing only the specified columns.\n",
    "\n",
    "> **Selecting Rows Based on a Condition**\n",
    ">\n",
    "> We showcase row selection based on a condition. Specifically, we filter rows where the 'Age' column is greater than 12, resulting in a DataFrame named `teens` containing only rows that meet the condition.\n",
    "\n",
    "> **Using `.loc` and `.iloc` Methods**\n",
    ">\n",
    "> We introduce the use of the `.loc` and `.iloc` methods for precise data selection. With `.iloc`, we select rows 1 and 2 along with all columns. With `.loc`, we demonstrate selecting data based on index labels and column names.\n",
    "\n",
    "> **Note**: In this example, integer index labels are used with `.loc`, but pandas also supports label-based indexing for custom index labels.\n",
    "\n",
    "These operations illustrate how to effectively extract and manipulate data within a pandas DataFrame, allowing for flexible data analysis and manipulation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9941258a-e3fb-48e7-b308-7f861960dae5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Selecting a single column\n",
    "# This creates a Series\n",
    "only_names = df_from_list['Name']\n",
    "print(\"Selected single column 'Name' as a Series:\")\n",
    "print(only_names)\n",
    "print(f\"The type is: {type(only_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9986c075-ee41-4d90-a3ac-cb8d2d8b9f32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Selecting specific columns\n",
    "# This creates a DataFrame\n",
    "subset = df_from_list[['Name', 'Age']]\n",
    "print(\"\\nSelected specific columns 'Name' and 'Age' as a DataFrame:\")\n",
    "print(subset)\n",
    "print(f\"The type is: {type(subset)}\")\n",
    "df_from_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6deb513-76b3-4ef5-a06d-e02751628c26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Selecting rows based on a condition\n",
    "teens = df_from_list[df_from_list['Age'] > 12]\n",
    "print(\"\\nSelected rows where 'Age' is greater than 12:\")\n",
    "print(teens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7523c589-e101-4c48-a37e-598108b7061e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using the `.loc` and `.iloc` methods for more precise selection\n",
    "# Selecting rows 1 and 2, and all columns\n",
    "subset = df_from_list.iloc[1:3, :2]\n",
    "print(\"\\nSelected rows 1 and 2, and all columns using .iloc:\")\n",
    "print(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6dc3c4-40d6-4664-beff-886adcfa999c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Selecting data based on index label and column names using `.loc`\n",
    "# Note: In this case, we are using integer index labels.\n",
    "subset_label = df_from_list.loc[1:2, 'Name':'Doubled Age']\n",
    "print(\"\\nSelected data based on index label and column names using .loc:\")\n",
    "print(subset_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e937b12-fabd-421d-9c60-16230fdfc822",
   "metadata": {},
   "source": [
    "# Aggregating and Grouping Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ede81d9-6aee-4249-a395-fcb652ac9466",
   "metadata": {},
   "source": [
    "## Basic Aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549e8fac-3a0b-4173-84e5-b21385a30e86",
   "metadata": {},
   "source": [
    "> **Overview**\n",
    ">\n",
    "> In this markdown block, we explore the calculation of basic statistics on the \"Age\" column within a pandas DataFrame. The code block demonstrates how to compute measures such as the mean, sum, maximum, minimum, median, and standard deviation of age values in the DataFrame.\n",
    "\n",
    "> **Calculating Basic Statistics**\n",
    ">\n",
    "> We start by calculating the following basic statistics on the \"Age\" column:\n",
    ">\n",
    "> - Mean Age: This is the average age of the individuals in the DataFrame.\n",
    "> - Sum of Ages: This represents the total sum of ages across all individuals.\n",
    "> - Oldest Age: This indicates the maximum age value in the \"Age\" column.\n",
    ">\n",
    "> These statistics provide a quick overview of the age distribution within the dataset.\n",
    "\n",
    "> **Calculating Additional Statistics**\n",
    ">\n",
    "> In addition to the basic statistics, we calculate the following additional statistics:\n",
    ">\n",
    "> - Youngest Age: This corresponds to the minimum age value in the \"Age\" column.\n",
    "> - Median Age: The median represents the middle value when the ages are sorted in ascending order.\n",
    "> - Standard Deviation of Age: This measures the variability or dispersion of age values around the mean.\n",
    ">\n",
    "> These additional statistics provide further insights into the age distribution, including measures of central tendency and data spread.\n",
    "\n",
    "> **Note**: Pandas provides convenient methods to calculate these statistics directly from a DataFrame column, making it easy to gain insights into your data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59090d19-c250-43d9-9508-ff1e5fd53b5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculating basic statistics on the \"Age\" column\n",
    "mean_age = df_from_list['Age'].mean()\n",
    "sum_age = df_from_list['Age'].sum()\n",
    "max_age = df_from_list['Age'].max()\n",
    "\n",
    "print(f\"Mean Age: {mean_age}\")\n",
    "print(f\"Sum of Ages: {sum_age}\")\n",
    "print(f\"Oldest Age: {max_age}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285709d9-ac6d-4c44-8caa-92b7a45c0643",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculating additional statistics\n",
    "min_age = df_from_list['Age'].min()\n",
    "median_age = df_from_list['Age'].median()\n",
    "std_dev_age = df_from_list['Age'].std()\n",
    "\n",
    "print(f\"Youngest Age: {min_age}\")\n",
    "print(f\"Median Age: {median_age}\")\n",
    "print(f\"Standard Deviation of Age: {std_dev_age}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8f0193-4e6b-4a5e-b272-d642da48e013",
   "metadata": {},
   "source": [
    "## Grouping and Aggregating Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b80b284-6c1f-4e3d-903f-fc0c878cd952",
   "metadata": {
    "tags": []
   },
   "source": [
    "> **Overview**\n",
    ">\n",
    "> In this markdown block, we explore data grouping and aggregation techniques in pandas using a sample dataset. The code block demonstrates how to create groups based on a specific column ('City' in this case) and perform calculations within each group, such as calculating mean ages, counting individuals, and summing ages.\n",
    "\n",
    "> **Creating Groups and Calculating Mean Age**\n",
    ">\n",
    "> We start by adding a 'City' column to the DataFrame `df_from_list`. This column represents the city of residence for each individual. Next, we use the `.groupby()` method to group the data by the 'City' column. We then calculate the mean age within each city using `.mean()`. The result is a DataFrame named `grouped_data` that shows the mean age for each city.\n",
    "\n",
    "> **Counting Individuals in Each City**\n",
    ">\n",
    "> Additionally, we use the `.value_counts()` method to count the number of individuals in each city. The result is a Series named `count_data` that provides a count of individuals in each city.\n",
    "\n",
    "> **Summing Ages Within Each City**\n",
    ">\n",
    "> To further demonstrate grouping and aggregation, we sum the ages within each city using the `.groupby()` method and `.sum()`. This operation yields a Series named `sum_data` that shows the total age sum within each city.\n",
    "\n",
    "> **Grouping and Aggregating Data**\n",
    ">\n",
    "> Grouping and aggregating data is a powerful technique in data analysis, allowing you to gain insights into various aspects of your dataset based on specific criteria. These operations are commonly used for summarizing data and exploring patterns within subgroups.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15693c5-d781-4163-9c5b-359393f23a6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Adding cities to the `df_from_list` DataFrame\n",
    "df_from_list['City'] = ['NY', 'LA', 'NY']\n",
    "\n",
    "# Grouping by city and calculating mean age within each city\n",
    "grouped_data = df_from_list.groupby('City')['Age'].mean()\n",
    "print(\"Grouped data with mean age within each city:\")\n",
    "print(grouped_data)\n",
    "\n",
    "# Counting the number of individuals in each city\n",
    "count_data = df_from_list['City'].value_counts()\n",
    "print(\"\\nCount of individuals in each city:\")\n",
    "print(count_data)\n",
    "\n",
    "# Summing the ages within each city\n",
    "sum_data = df_from_list.groupby('City')['Age'].sum()\n",
    "print(\"\\nTotal age sum within each city:\")\n",
    "print(sum_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca01449-d417-4fcc-b406-00281bc39d3a",
   "metadata": {},
   "source": [
    "# Merging and Joining DataFrames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e9a056-85d3-4bfc-b6be-1e910104efea",
   "metadata": {},
   "source": [
    "## Combining DataFrames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87ffc90-6258-4166-9233-ac72a2cfe8a1",
   "metadata": {},
   "source": [
    "> **Overview**\n",
    ">\n",
    "> In this markdown block, we explore how to combine and merge two pandas DataFrames based on a common column using different types of joins. The code block demonstrates various types of joins, including outer join, inner join, left join, and right join.\n",
    "\n",
    "> **Merging DataFrames Based on a Common Key**\n",
    ">\n",
    "> We start by creating two sample DataFrames, `df1` and `df2`, each containing a 'Key' column. These DataFrames will be used for merging.\n",
    "\n",
    "> **Outer Join**\n",
    ">\n",
    "> We use the `pd.merge()` function to merge `df1` and `df2` based on the 'Key' column using an outer join. An outer join includes all unique values from both DataFrames, filling in missing values with NaN where necessary. The result is stored in the `merged` DataFrame.\n",
    "\n",
    "> **Inner Join**\n",
    ">\n",
    "> Next, we perform an inner join, which includes only the values that exist in both DataFrames. The result is stored in the `inner_merged` DataFrame.\n",
    "\n",
    "> **Left Join**\n",
    ">\n",
    "> A left join includes all values from the left DataFrame (`df1`) and the matching values from the right DataFrame (`df2`). Missing values from the right DataFrame are filled with NaN. The result is stored in the `left_merged` DataFrame.\n",
    "\n",
    "> **Right Join**\n",
    ">\n",
    "> Conversely, a right join includes all values from the right DataFrame (`df2`) and the matching values from the left DataFrame (`df1`). Missing values from the left DataFrame are filled with NaN. The result is stored in the `right_merged` DataFrame.\n",
    "\n",
    "> **Note**: Joining DataFrames is a common operation when working with relational data, allowing you to combine data from different sources based on shared keys.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1c85e7-7af0-4385-ba21-c3cb6aa9fd1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating two DataFrames for the demonstration\n",
    "df1 = pd.DataFrame({'Key': ['A', 'B', 'C'], 'Value1': [1, 2, 3]})\n",
    "df2 = pd.DataFrame({'Key': ['A', 'B', 'D'], 'Value2': [4, 5, 6]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0b097a0a-e26d-4ffb-b066-c4e8e5021149",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged DataFrames using an outer join:\n",
      "  Key  Value1  Value2\n",
      "0   A     1.0     4.0\n",
      "1   B     2.0     5.0\n",
      "2   C     3.0     NaN\n",
      "3   D     NaN     6.0\n"
     ]
    }
   ],
   "source": [
    "# Merging them based on the \"Key\" column using an outer join\n",
    "merged = pd.merge(df1, df2, on='Key', how='outer')\n",
    "print(\"Merged DataFrames using an outer join:\")\n",
    "print(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6db1eab2-9a79-4141-8dae-8a38cfcdf255",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merged DataFrames using an inner join:\n",
      "  Key  Value1  Value2\n",
      "0   A       1       4\n",
      "1   B       2       5\n"
     ]
    }
   ],
   "source": [
    "# Performing an inner join\n",
    "inner_merged = pd.merge(df1, df2, on='Key', how='inner')\n",
    "print(\"\\nMerged DataFrames using an inner join:\")\n",
    "print(inner_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d96801bb-380f-4a00-8b4d-2586f794a909",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merged DataFrames using a left join:\n",
      "  Key  Value1  Value2\n",
      "0   A       1     4.0\n",
      "1   B       2     5.0\n",
      "2   C       3     NaN\n"
     ]
    }
   ],
   "source": [
    "# Performing a left join\n",
    "left_merged = pd.merge(df1, df2, on='Key', how='left')\n",
    "print(\"\\nMerged DataFrames using a left join:\")\n",
    "print(left_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "975092de-3926-4784-8f6e-edaa32aba713",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merged DataFrames using a right join:\n",
      "  Key  Value1  Value2\n",
      "0   A     1.0       4\n",
      "1   B     2.0       5\n",
      "2   D     NaN       6\n"
     ]
    }
   ],
   "source": [
    "# Performing a right join\n",
    "right_merged = pd.merge(df1, df2, on='Key', how='right')\n",
    "print(\"\\nMerged DataFrames using a right join:\")\n",
    "print(right_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97c3fbd-282b-4bd0-b475-74eb83f4acd1",
   "metadata": {},
   "source": [
    "This merge uses the 'outer' method, ensuring all keys from both DataFrames are included. Other methods like 'inner', 'left', and 'right' give you control over which keys to retain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91b130c-40ca-4f21-9f26-08526682ad4a",
   "metadata": {},
   "source": [
    "> **Example: Right Join of Employees and Departments Tables**\n",
    ">\n",
    "> Consider two SQL-like tables, `Employees` and `Departments`, and perform a right join between them based on a common key, which is the department ID (`DeptID`).\n",
    ">\n",
    "> **Employees Table:**\n",
    ">\n",
    "> | EmpID | EmpName  | DeptID |\n",
    "> |-------|----------|--------|\n",
    "> | 1     | Alice    | 101    |\n",
    "> | 2     | Bob      | 102    |\n",
    "> | 3     | Charlie  | 103    |\n",
    "> | 4     | Dave     | 104    |\n",
    ">\n",
    "> **Departments Table:**\n",
    ">\n",
    "> | DeptID | DeptName    |\n",
    "> |--------|-------------|\n",
    "> | 101    | HR          |\n",
    "> | 103    | Finance     |\n",
    "> | 105    | Marketing   |\n",
    "> | 106    | Operations  |\n",
    ">\n",
    "> In this example, we want to perform a right join between the `Employees` and `Departments` tables on the `DeptID` column. The result will include all rows from the `Departments` table and matching rows from the `Employees` table. If there is no match in the `Employees` table, the corresponding columns will be filled with NaN.\n",
    ">\n",
    "> **Resultant Table (After Right Join):**\n",
    ">\n",
    "> | EmpID | EmpName  | DeptID | DeptName    |\n",
    "> |-------|----------|--------|-------------|\n",
    "> | 1     | Alice    | 101    | HR          |\n",
    "> | NaN   | NaN      | 105    | Marketing   |\n",
    "> | 3     | Charlie  | 103    | Finance     |\n",
    "> | NaN   | NaN      | 106    | Operations  |\n",
    ">\n",
    "> In the resultant table:\n",
    ">\n",
    "> - The rows with `EmpID` 1 and 3 are matching between the `Employees` and `Departments` tables based on the `DeptID` column, so they appear in the merged table.\n",
    "> \n",
    "> - Rows with `DeptID` 105 and 106 exist only in the `Departments` table, so they also appear in the merged table. However, the corresponding columns for `EmpID` and `EmpName` from the `Employees` table are filled with NaN because there are no matching employees in those departments.\n",
    ">\n",
    "> This is an example of a right join, which ensures that all unique values from the right table (`Departments`) are included in the merged table and brings in matching values from the left table (`Employees`) while filling missing values with NaN when there is no match in the left table.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
