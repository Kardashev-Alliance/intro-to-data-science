{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb71d826-9d46-408f-bdb1-42ef4dc9abcc",
   "metadata": {},
   "source": [
    "# Module 7: Real-World Data Analysis Projects\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719c253f-9f71-4f4c-aab6-0dddfdb18462",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Section 1: Data Loading and Initial Exploration\n",
    "\n",
    "**Objective:**  \n",
    "In this section, we initiate our guided analysis project by loading the California Housing Prices dataset and conducting an initial exploration of its structure.\n",
    "\n",
    "**Overview:**  \n",
    "To start our analysis, we need to obtain the dataset and understand its contents. We load the dataset using scikit-learn's `fetch_california_housing` function with the option `as_frame=True` to fetch it as a pandas DataFrame. This allows us to work with the data efficiently.\n",
    "\n",
    "Next, we create a DataFrame named `df` from the dataset, including feature names and the target variable. By doing this, we can easily manipulate and analyze the data.\n",
    "\n",
    "To ensure everything is functioning correctly and to get a sense of the dataset's structure, we display the first few rows using `df.head()`. This initial exploration helps us see what features are available and gain an overall understanding of the dataset's format.\n",
    "\n",
    "By the end of this section, we will have a DataFrame ready for further analysis, setting the foundation for our guided project on California housing prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54564f72-95a9-4226-837e-faba7ed7c7b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Load the California Housing Prices dataset\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "\n",
    "# Create a DataFrame from the dataset\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['Target'] = data.target  # Adding the target variable to the DataFrame\n",
    "\n",
    "# Display the first few rows of the dataset to understand its structure\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aba2762-73b7-4e35-8790-b91da187dea3",
   "metadata": {},
   "source": [
    "## Glossary of Features in the California Housing Dataset\n",
    "\n",
    "Let's explore the features present in the California Housing dataset to understand their meaning and relevance:\n",
    "\n",
    "1. **MedInc (Median Income):** This feature represents the median income of households in a specific geographical area (in units of tens of thousands of dollars). It is an important socioeconomic indicator and can influence housing prices.\n",
    "\n",
    "2. **HouseAge (Housing Median Age):** HouseAge is the median age of houses within a block, expressed in years. Older houses might be less expensive compared to newer ones.\n",
    "\n",
    "3. **AveRooms (Average Rooms):** AveRooms is the average number of rooms per housing unit in a given area. It can provide insights into the housing density and size.\n",
    "\n",
    "4. **AveBedrms (Average Bedrooms):** AveBedrms represents the average number of bedrooms per housing unit. It gives an idea of the typical bedroom count in the region.\n",
    "\n",
    "5. **Population:** Population indicates the number of people residing in a geographical area. It can impact housing demand and prices.\n",
    "\n",
    "6. **AveOccup (Average Occupancy):** AveOccup is the average household occupancy, which is the ratio of the population to the number of households. It can provide insights into housing density.\n",
    "\n",
    "7. **Latitude:** Latitude specifies the geographic latitude of the location, which can be important for regional climate and desirability.\n",
    "\n",
    "8. **Longitude:** Longitude represents the geographic longitude of the location, providing information about the geographical positioning.\n",
    "\n",
    "9. **Target (Median House Value):** The Target variable represents the median house value in a specific area (in units of hundreds of thousands of dollars). This is the target variable for regression tasks, and it's what we aim to predict.\n",
    "\n",
    "### Understanding the Dataset:\n",
    "\n",
    "- **MedInc (Median Income)** and **HouseAge (Housing Median Age)** may be correlated with housing prices. Higher median income areas might have more expensive houses, and newer houses might be pricier.\n",
    "\n",
    "- **AveRooms (Average Rooms)** and **AveBedrms (Average Bedrooms)** provide information about the size and layout of houses. Areas with larger average rooms or bedrooms might have more spacious properties.\n",
    "\n",
    "- **Population** and **AveOccup (Average Occupancy)** are related to housing density. High population areas with lower occupancy rates may indicate larger properties or lower demand for housing.\n",
    "\n",
    "- **Latitude** and **Longitude** offer geographical information that can be used for spatial analysis and understanding regional variations in housing prices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3e4867-e6d5-4055-8cff-f6d51cd34f0f",
   "metadata": {},
   "source": [
    "### Section 2: Data Cleaning and Initial Analysis\n",
    "\n",
    "**Objective:**  \n",
    "In this section, we perform data cleaning and conduct an initial analysis of the California Housing Prices dataset.\n",
    "\n",
    "**Steps:**  \n",
    "\n",
    "**1. Checking for Missing Values:**  \n",
    "We begin by checking for any missing values in the dataset. Missing values can significantly impact our analysis and modeling. By calculating the sum of missing values for each feature using `df.isnull().sum()`, we ensure data completeness.\n",
    "\n",
    "**2. Handling Missing Values (if any):**  \n",
    "Fortunately, in this dataset, no missing values are found. Therefore, there is no need for further data cleaning or imputation.\n",
    "\n",
    "**3. Summary Statistics:**  \n",
    "To gain insights into the distribution and central tendencies of the numerical features, we calculate summary statistics using `df.describe()`. This step provides us with key statistical information, including mean, standard deviation, minimum, maximum, and quartiles.\n",
    "\n",
    "**4. Feature Distribution Visualization:**  \n",
    "Visualizing the distribution of features is essential to understand the characteristics of the data. We create a grid of histograms for all numerical features, showing the frequency distribution of each. This helps us identify patterns, potential outliers, and the overall shape of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877672d2-e4d0-4576-8833-6353802722dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "if not df.isnull().any().any():\n",
    "    print(\"There are no missing values in this dataset\")\n",
    "else:\n",
    "    print(f\"There are {df.isnull().sum()} missing values in this dataset\")\n",
    "\n",
    "# Data Cleaning: Handling Missing Values (if any)\n",
    "# No missing values found in this dataset, so no further data cleaning is needed\n",
    "\n",
    "# Summary statistics for numerical features\n",
    "summary_stats = df.describe()\n",
    "print(summary_stats)\n",
    "\n",
    "# Feature Distribution Visualization\n",
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "# Plot histograms for all numerical features\n",
    "for i, feature in enumerate(data.feature_names):\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    plt.hist(df[feature], bins=30, edgecolor='k', alpha=0.7)\n",
    "    plt.title(f'Distribution of {feature}')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b0a932-dce7-42f6-aeea-8d008ddeab16",
   "metadata": {},
   "source": [
    "### Section 3: Exploring Feature Relationships\n",
    "\n",
    "**Objective:**  \n",
    "In this section, we explore the relationships between numerical features in the California Housing Prices dataset by creating a correlation matrix and visualizing it as a heatmap.\n",
    "\n",
    "**Steps:**  \n",
    "\n",
    "**1. Calculating the Correlation Matrix:**  \n",
    "Understanding how features are related to each other is crucial in data analysis. To do this, we calculate the correlation matrix using `df.corr()`. This matrix provides correlation coefficients for all pairs of numerical features, showing the strength and direction of their linear relationships.\n",
    "\n",
    "**2. Creating a Heatmap for Visualization:**  \n",
    "A heatmap is an effective way to visually represent the correlation matrix. We create a heatmap using the Seaborn library with the `sns.heatmap()` function. It allows us to color-code the correlation values, making it easier to identify strong positive (closer to 1) and negative (closer to -1) correlations. The `annot=True` parameter adds correlation values to the heatmap cells, improving interpretability.\n",
    "\n",
    "**3. Interpretation:**  \n",
    "The resulting heatmap provides insights into how numerical features in the dataset are correlated. Darker squares indicate stronger correlations, while lighter squares suggest weaker or no correlations. This visualization helps identify potential multicollinearity (high correlations between independent variables), which can impact predictive modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cd5b00-f94e-4520-b96b-e4c883109664",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix for numerical columns\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Create a heatmap to visualize correlations\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bd9ab4-cac7-41bd-b3c7-ff6ce0c7aeac",
   "metadata": {},
   "source": [
    "## Interpreting the Correlation Heatmap\n",
    "\n",
    "Now that we have generated the correlation heatmap for the California Housing dataset, let's interpret the heatmap and gain insights into the relationships between numerical features.\n",
    "\n",
    "### Interpretation:\n",
    "\n",
    "The heatmap provides a visual representation of the correlation coefficients between numerical features. Each square in the heatmap represents the correlation between two features, and the color intensity indicates the strength and direction of the correlation:\n",
    "\n",
    "- **Positive Correlation:** Features that have a positive correlation show a tendency to move together. If one feature increases, the other tends to increase as well, and vice versa. Positive correlations are represented by dark red squares in the heatmap.\n",
    "\n",
    "- **Negative Correlation:** Features with a negative correlation move in opposite directions. When one feature increases, the other tends to decrease. Negative correlations are shown as dark blue squares.\n",
    "\n",
    "- **No Correlation:** Features that have little to no correlation appear as light or white squares in the heatmap. This indicates that changes in one feature do not significantly affect the other.\n",
    "\n",
    "### Using the Heatmap:\n",
    "\n",
    "- Identifying Relationships: Examine the dark red and dark blue squares to identify pairs of features that have strong correlations. These relationships can be crucial when selecting features for modeling.\n",
    "\n",
    "- Multicollinearity: Be cautious about strong positive or negative correlations between features, as they may indicate multicollinearity. Multicollinearity can affect the stability and interpretability of predictive models.\n",
    "\n",
    "- Feature Selection: Use the heatmap to guide feature selection by focusing on features that are strongly correlated with the target variable or have meaningful correlations with other features.\n",
    "\n",
    "- Data Insights: The heatmap can provide valuable insights into the dataset's structure and highlight which features may have predictive power.\n",
    "\n",
    "Remember that correlation does not imply causation. A strong correlation between two features does not necessarily mean one causes the other; it indicates a statistical relationship. Use these insights to inform your data analysis and modeling decisions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018431b5-dd94-4bd1-b584-78c6dd4c4a4c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Section 4: Polynomial Regression for Nonlinear Analysis\n",
    "\n",
    "Polynomial regression is a regression technique used when the relationship between the independent variable(s) and the target variable is nonlinear. In this context, we're applying polynomial regression to the California housing dataset to explore potential nonlinear relationships between median income (MedInc) and median house values (Target).\n",
    "\n",
    "#### Code Explanation:\n",
    "\n",
    "- We load the California Housing Prices dataset, similar to our previous exploration.\n",
    "- We select \"Median Income\" as our predictor variable (X) and \"Median House Value\" as our target variable (y).\n",
    "    - **Predictor Variable** is often referred to as an independent variable or feature, is a data attribute or input used in statistical or machine learning models to predict or explain changes in the target variable.\n",
    "- We specify the degree of the polynomial, which determines the complexity of the model. Here, we use a degree of 2, which means we'll fit a quadratic (second-degree) polynomial.\n",
    "    - **Determining the Polynomial Degree** The degree of the polynomial in polynomial regression is determined by us, the data analysts or scientists. It represents the order of the polynomial equation that will be fitted to the data. For example, a degree of 2 corresponds to a quadratic equation, degree 3 to a cubic equation, and so on.\n",
    "    - **Choosing the Right Polynomial Degree** Selecting the appropriate polynomial degree is crucial. A lower-degree polynomial may underfit the data, meaning it won't capture complex relationships, while a higher-degree polynomial may overfit the data, fitting noise in the dataset rather than the true underlying pattern. \n",
    "    - **In Our Example**, we specified a degree of 2, which means we chose to fit a quadratic (second-degree) polynomial to our data. This allows our model to capture a curved relationship between the predictor variable (e.g., \"Median Income\") and the target variable (e.g., \"House Value\"). The choice of degree should be based on a balance between model complexity and its ability to represent the underlying patterns in the data.\n",
    "\n",
    "- We create a polynomial regression model using `PolynomialFeatures` and `LinearRegression` from Scikit-Learn. This model transforms our predictor variable into polynomial features and fits a linear regression model to it.\n",
    "    - **Polynomial Regression Model** Polynomial regression is a regression technique used to model relationships between variables that aren't linear. It allows us to capture nonlinear patterns by transforming the predictor variable(s) into polynomial features.\n",
    "    - **PolynomialFeatures** PolynomialFeatures is a preprocessing step provided by Scikit-Learn. It takes an original predictor variable and transforms it into polynomial features. For example, if we have a second-degree polynomial, it will create new features like \"Median Income squared\" and \"Median Income cubed.\"\n",
    "    - **LinearRegression** LinearRegression, also from Scikit-Learn, is used in polynomial regression to fit a linear equation to the polynomial features generated by PolynomialFeatures. Despite the polynomial features, LinearRegression applies a linear model in terms of coefficients.\n",
    "    - **Predictor Variable** The predictor variable, in this context, is the original variable we want to use for making predictions. In our example, it's \"Median Income.\" PolynomialFeatures takes this predictor variable and generates polynomial terms from it.\n",
    "    - **Fits to a Linear Regression Model**\n",
    "After transforming the predictor variable into polynomial features, we use LinearRegression to fit a linear equation to these polynomial features. This linear equation, with its coefficients, serves as our model. It allows us to make predictions based on the polynomial terms of the original predictor variable, capturing nonlinear relationships within the data.\n",
    "\n",
    "- After fitting the model, we make predictions and plot the actual house prices (scatter) and the polynomial regression curve (red line).\n",
    "\n",
    "#### Interpretation:\n",
    "\n",
    "- Polynomial regression can capture more complex relationships than linear regression. In this case, we aim to capture potential nonlinear trends in the relationship between median income and median house values.\n",
    "- The resulting curve visually represents the fitted polynomial regression model. In the plot, you can observe how the polynomial curve attempts to capture the nonlinear patterns in the data.\n",
    "- Polynomial regression allows us to explore whether higher degrees of polynomials (e.g., cubic, quartic) might provide a better fit to the data and reveal more complex relationships.\n",
    "\n",
    "Using polynomial regression, we can uncover nonlinear insights within the dataset, complementing our initial linear analysis. This technique is valuable when linear models fail to capture the underlying relationships effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b33cca4-dcda-4034-a87f-38b829442f6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Load the California Housing Prices dataset\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "\n",
    "# Create a DataFrame from the dataset\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['Target'] = data.target  # Adding the target variable to the DataFrame\n",
    "\n",
    "# Selecting predictor and target variables\n",
    "X = df[['MedInc']]\n",
    "y = df['Target']\n",
    "\n",
    "# Perform Polynomial Regression\n",
    "degree = 2  # Degree of the polynomial\n",
    "polyreg = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "polyreg.fit(X, y)\n",
    "\n",
    "# Predict housing prices\n",
    "y_pred = polyreg.predict(X)\n",
    "\n",
    "# Plot the results\n",
    "plt.scatter(X, y, label='Actual Prices', s=10)\n",
    "plt.plot(X, y_pred, color='red', label='Polynomial Regression (Degree 2)')\n",
    "plt.xlabel('Median Income')\n",
    "plt.ylabel('Median House Value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcdec3e-9218-41f6-bc02-0930c4f074d1",
   "metadata": {},
   "source": [
    "The graph illustrates the relationship between the median income (on the x-axis) and the median house value (on the y-axis) for the California Housing Prices dataset. The red line represents the polynomial regression model (degree 2) that has been fitted to the data.\n",
    "\n",
    "Here's what you should understand from the graph:\n",
    "\n",
    "1. **Data Distribution**: The scattered blue points on the graph represent the actual median house values for different levels of median income. These points give you an idea of how the data is distributed across the income and housing value ranges.\n",
    "\n",
    "2. **Regression Line**: The red curve represents the polynomial regression line, specifically a quadratic (second-degree) polynomial. It shows how the model predicts median house values based on median income. The curve captures the overall trend and relationship between these two variables.\n",
    "\n",
    "3. **Model Fit**: The curve attempts to fit the data points as closely as possible, and its shape is determined by the degree of the polynomial chosen for regression. In this case, a quadratic polynomial (degree 2) has been used, resulting in a parabolic curve.\n",
    "\n",
    "4. **Predictive Power**: The polynomial regression model can be used for predictions. Given a median income value, you can use the curve to estimate the corresponding median house value. For example, if you have a median income of $4, the model can predict the expected median house value based on the curve's position at that point.\n",
    "\n",
    "5. **Trend**: You can observe the general trend that as median income increases, median house value tends to rise as well, which is an intuitive relationship. The curve captures this trend and the associated variability in house values.\n",
    "\n",
    "6. **Residuals**: The differences between the actual data points (blue) and the points on the curve (red) are called residuals. They represent the model's errors or how well it fits the data. Smaller residuals indicate a better fit.\n",
    "\n",
    "In summary, this graph helps you visualize how a polynomial regression model fits the data and how it can be used to make predictions based on the relationship between median income and median house value. It's a valuable tool for understanding and modeling such relationships in real-world datasets.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
